{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3BhZgBsX9J2"
   },
   "source": [
    "## üìò Introducci√≥n a los LLMs y al Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glosario de Conceptos\n",
    "\n",
    "#### ¬øQu√© es un LLM y c√≥mo funciona?\n",
    "\n",
    "Un **LLM (Large Language Model)** es un modelo de lenguaje basado en aprendizaje profundo, entrenado con grandes vol√∫menes de texto para entender y generar lenguaje humano. Est√° dise√±ado para realizar tareas como:\n",
    "\n",
    "- Completar textos\n",
    "- Traducir idiomas\n",
    "- Responder preguntas\n",
    "- Generar c√≥digo\n",
    "- Resumir contenido\n",
    "\n",
    "Los LLMs se basan principalmente en la **arquitectura Transformer**, donde la atenci√≥n es el mecanismo clave para procesar texto en paralelo y comprender el contexto.\n",
    "\n",
    "**Funcionamiento general:**\n",
    "\n",
    "1. **Input (Prompt):** El usuario proporciona una instrucci√≥n o contexto.\n",
    "2. **Tokenizaci√≥n:** El texto se convierte en tokens (fragmentos de palabras).\n",
    "3. **Procesamiento:** El modelo calcula relaciones entre tokens usando capas de atenci√≥n.\n",
    "4. **Output:** El modelo genera una respuesta (texto, c√≥digo, etc.) como continuaci√≥n l√≥gica del prompt.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Fundamentos del Prompting\n",
    "\n",
    "El **prompting** es el arte de dise√±ar entradas efectivas para obtener respuestas √∫tiles de un modelo LLM. Existen tres enfoques comunes:\n",
    "\n",
    "### ‚úÖ Zero-shot Prompting\n",
    "\n",
    "Consiste en hacer una solicitud directa al modelo **sin proporcionar ejemplos previos**.\n",
    "\n",
    "```text\n",
    "Prompt:\n",
    "\"Resume el siguiente texto en una oraci√≥n.\"\n",
    "```\n",
    "\n",
    "### ‚úÖ Few-shot Prompting\n",
    "Se proporciona al modelo uno o m√°s ejemplos antes de realizar la solicitud. Esto le ayuda a entender el formato o la l√≥gica deseada.\n",
    "\n",
    "```text\n",
    "Prompt:\n",
    "Texto: \"El cielo est√° nublado y podr√≠a llover.\"\n",
    "Resumen: \"Podr√≠a llover pronto.\"\n",
    "\n",
    "Texto: \"Hace calor y hay mucho sol.\"\n",
    "Resumen:\n",
    "```\n",
    "\n",
    "### ‚úÖ Chain-of-Thought (CoT) Prompting\n",
    "Implica pedirle al modelo que razone paso a paso, como si pensara en voz alta. Esto permite respuestas m√°s precisas para tareas de l√≥gica, c√°lculo o an√°lisis.\n",
    "\n",
    "```text\n",
    "Pregunta:\n",
    "Si Mar√≠a tiene 5 manzanas y le da 2 a Juan, ¬øcu√°ntas le quedan?\n",
    "\n",
    "Respuesta:\n",
    "Primero, Mar√≠a tiene 5 manzanas.\n",
    "Luego, le da 2 a Juan.\n",
    "Entonces, le quedan 3 manzanas.\n",
    "```\n",
    "\n",
    "### üî† Tokens, Embeddings y Generaci√≥n de Texto\n",
    "#### üéØ Tokens\n",
    "\n",
    "Los LLMs no procesan texto directamente, sino que lo dividen en tokens, que pueden ser palabras, s√≠labas o incluso fragmentos de palabras.\n",
    "\n",
    "```text\n",
    "Texto: \"Hola mundo\"\n",
    "Tokens: [ \"Hola\", \" mundo\" ]\n",
    "```\n",
    "\n",
    "#### üß¨ Embeddings\n",
    "\n",
    "Cada token se transforma en un vector num√©rico conocido como embedding, que captura informaci√≥n sem√°ntica. Los embeddings permiten al modelo entender similitudes y relaciones entre palabras.\n",
    "\n",
    "####  ‚úçÔ∏è Generaci√≥n de Texto\n",
    "\n",
    "La generaci√≥n de texto se realiza token por token, prediciendo la siguiente palabra m√°s probable seg√∫n el contexto anterior. El resultado final es una secuencia coherente y contextual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9bygXR4W7qL",
    "outputId": "6d872eb1-8684-4238-d16b-d5544223a41a"
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from getpass import getpass\n",
    "GEMINI_API_KEY = getpass('Enter API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rjusJKdYXkKv",
    "outputId": "7e18ef01-b102-4ece-c7b8-5dc247d0fd26"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configura tu API key (aseg√∫rate de definir GEMINI_API_KEY correctamente antes)\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Usa el modelo correcto\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "# Llama a generate_content correctamente\n",
    "response = model.generate_content(\"Act√∫a como profe de ingl√©s y explica el pasado imperfecto con ejemplos.\")\n",
    "\n",
    "# Imprime el resultado\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLvMao4EZggi"
   },
   "source": [
    "## üß© Modelos LLM Multimodales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¬øQu√© son?\n",
    "\n",
    "Los **LLMs multimodales** son modelos de lenguaje capaces de procesar y generar contenido no solo en texto, sino tambi√©n a partir de otras modalidades como **im√°genes**, **audio** y **video**. Esto permite una interacci√≥n m√°s natural y enriquecida con la inteligencia artificial.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß™ Ejemplos de uso con Gemini\n",
    "\n",
    "- üñºÔ∏è **Descripci√≥n de im√°genes**: interpretar el contenido visual y generar descripciones en lenguaje natural.\n",
    "- üîä **Transcripci√≥n de audio**: convertir voz a texto de manera autom√°tica.\n",
    "- üòä **An√°lisis de sentimiento**: detectar emociones y tono a partir de texto o voz.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Par√°metros importantes\n",
    "\n",
    "Al usar LLMs multimodales, se pueden ajustar varios **par√°metros de configuraci√≥n** para optimizar resultados:\n",
    "\n",
    "- `temperature`: controla la creatividad de las respuestas (valores bajos generan respuestas m√°s precisas).\n",
    "- `model`: selecci√≥n del modelo adecuado (ej. `gemini-2.0-pro`, `gemini-2.0-flash`, etc.).\n",
    "- `max_output_tokens`: define la longitud m√°xima de la respuesta generada.\n",
    "\n",
    "---\n",
    "\n",
    "#### üí° Ventajas clave\n",
    "\n",
    "- Interacci√≥n m√°s rica y contextual.\n",
    "- Permite soluciones inclusivas y accesibles.\n",
    "- Aplicaciones en educaci√≥n, salud, dise√±o, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîó Recurso √∫til\n",
    "\n",
    "- [Gemini Multimodal API](https://aistudio.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "dXnrpDwpYS1z",
    "outputId": "8854a53a-c151-4076-e493-8a854ff86e9a"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"Act√∫a como profe de ingl√©s, y dame ejemplos para mejorar vocabulario.\",\n",
    "    generation_config={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_output_tokens\": 500\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "XBjX4MLrQPcd",
    "outputId": "071238b9-b030-4b10-c856-5ba3e8dbb658"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "image_url = \"https://kinsta.com/es/wp-content/uploads/sites/8/2019/09/jpg-vs-jpeg.jpg\"\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "img_byte_arr = BytesIO()\n",
    "img.save(img_byte_arr, format='JPEG')\n",
    "img_bytes = img_byte_arr.getvalue()\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "# Send the prompt with image\n",
    "response = model.generate_content(\n",
    "    [\n",
    "        \"Describe la escena en la imagen usando palabras sencillas para que un estudiante principiante pueda entenderla.\",\n",
    "        {\"mime_type\": \"image/jpeg\", \"data\": img_bytes}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s8qxp7dUwMM"
   },
   "source": [
    "## üíª Generaci√≥n de C√≥digo con LLMs\n",
    "\n",
    "### Introducci√≥n\n",
    "\n",
    "Los **Modelos de Lenguaje de Gran Escala (LLMs)** no solo comprenden texto, tambi√©n pueden **generar c√≥digo** en m√∫ltiples lenguajes de programaci√≥n. Gracias a su capacidad de aprender patrones sint√°cticos y sem√°nticos, los LLMs son √∫tiles para tareas como:\n",
    "\n",
    "- Escribir funciones y scripts desde cero\n",
    "- Explicar fragmentos de c√≥digo\n",
    "- Traducir c√≥digo entre lenguajes\n",
    "- Automatizar tareas repetitivas\n",
    "- Resolver problemas paso a paso\n",
    "\n",
    "Uno de los principales beneficios es la **aceleraci√≥n del desarrollo**, especialmente para principiantes o para quienes buscan prototipar r√°pidamente. Sin embargo, es importante **verificar la validez y seguridad** del c√≥digo generado.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Actividad: Traductor palabra por palabra en Python\n",
    "\n",
    "#### Enunciado\n",
    "\n",
    "Usa **Gemini API** para generar un script en Python que:\n",
    "\n",
    "- Reciba como entrada una oraci√≥n en ingl√©s.\n",
    "- Devuelva la traducci√≥n **palabra por palabra** al espa√±ol.\n",
    "- Se asuma que el modelo es un **experto en Python** y proporcione una soluci√≥n funcional y clara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qZxcOeFNUvu9",
    "outputId": "a1a5e52a-08f4-4938-8138-4fb917768caa"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "response = model.generate_content(\"Eres un experto en python. Escribe un c√≥digo en Python que tome una oraci√≥n en ingl√©s y lo traduzca cada palabra al espa√±ol\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeB63iuYgBuV"
   },
   "source": [
    "### **Prompt Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kGAMA4vgFxV",
    "outputId": "eba598db-fab2-4511-a943-f5dcd352be4d"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Eres un experto en programaci√≥n, escribiendo c√≥digo limpio en python y escribes comentarios en cada l√≠nea de c√≥digo.\n",
    "a continuaci√≥n, {pregunta}.\n",
    "\"\"\"\n",
    "pregunta = \"Crea un diccionario\"\n",
    "prompt = prompt_template.format(pregunta=pregunta)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "JXb7HTRKg3kr",
    "outputId": "a785d2f9-92ef-44b6-ae99-60016c8adec4"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "id": "TNnljWMpi02I",
    "outputId": "4d36a597-4721-4edd-99da-ccee26232ca8"
   },
   "outputs": [],
   "source": [
    "#Implementaci√≥n de modelo que razona 'experimental gemini 2.0 flash'\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp')\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gz5abmeQi-ll",
    "outputId": "848ab5d1-a994-4948-d381-4b126866a047"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import google.generativeai as genai\n",
    "\n",
    "#Implementaci√≥n de modelo que razona 'experimental gemini 2.0 flash'\n",
    "prompt_template = \"\"\"\n",
    "Eres un experto en programaci√≥n, escribiendo c√≥digo limpio en python y escribes comentarios en cada l√≠nea de c√≥digo.\n",
    "Explicame este c√≥digo con detalle y como aplicarlo.\n",
    "a continuaci√≥n,\n",
    "{codigo}.\n",
    "\n",
    "El resultado deve estar en formato Markdown.\n",
    "\"\"\"\n",
    "\n",
    "codigo = \"\"\"{ # Inicia la definici√≥n del diccionario con una llave de apertura {.\n",
    "    \"clave_texto\": \"valor de ejemplo\", # Define el primer par clave-valor: \"clave_texto\" apunta a una cadena.\n",
    "    \"clave_numero\": 123,             # Define el segundo par clave-valor: \"clave_numero\" apunta a un n√∫mero entero.\n",
    "    \"clave_booleano\": True,          # Define el tercer par clave-valor: \"clave_booleano\" apunta a un valor booleano.\n",
    "    \"clave_lista\": [1, 2, 3]         # Define el cuarto par clave-valor: \"clave_lista\" apunta a una lista.\n",
    "} # Cierra la definici√≥n del diccionario con una llave de cierre }.\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt_template.format(codigo=codigo)\n",
    "\n",
    "print(prompt)\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp')\n",
    "response = model.generate_content(prompt)\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíª 3. Generaci√≥n de C√≥digo y Hugging Face\n",
    "\n",
    "## üßæ Generaci√≥n y explicaci√≥n de c√≥digo mediante prompts\n",
    "\n",
    "Los **Modelos de Lenguaje de Gran Escala (LLMs)** pueden generar fragmentos de c√≥digo fuente a partir de instrucciones en lenguaje natural. Esto se logra mediante **prompts** bien dise√±ados que indican al modelo qu√© tipo de c√≥digo se desea obtener. Por ejemplo, se puede pedir al modelo:\n",
    "\n",
    "- Que escriba una funci√≥n espec√≠fica.\n",
    "- Que traduzca c√≥digo de un lenguaje a otro.\n",
    "- Que explique paso a paso lo que hace un bloque de c√≥digo.\n",
    "\n",
    "Esta capacidad transforma al LLM en un asistente de programaci√≥n que puede ser √∫til tanto para principiantes como para desarrolladores avanzados.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ó Uso de la librer√≠a Transformers de Hugging Face\n",
    "\n",
    "La biblioteca **Transformers** de [Hugging Face](https://huggingface.co/transformers/) permite acceder y trabajar con una gran variedad de modelos preentrenados, incluyendo aquellos dise√±ados para tareas de generaci√≥n de c√≥digo como:\n",
    "\n",
    "- `CodeT5`\n",
    "- `StarCoder`\n",
    "- `GPT-2` y `GPT-Neo` con fine-tuning\n",
    "- `Phi`, `CodeGen`, entre otros\n",
    "\n",
    "Caracter√≠sticas clave de la librer√≠a:\n",
    "\n",
    "- Compatibilidad con PyTorch y TensorFlow.\n",
    "- Carga sencilla de modelos y tokenizadores.\n",
    "- Interfaz de alto nivel para inferencia y entrenamiento.\n",
    "- Integraci√≥n con APIs como Google Generative AI, OpenAI o DeepSpeed.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Ventajas y desaf√≠os del uso de LLMs en programaci√≥n\n",
    "\n",
    "### ‚úÖ Ventajas\n",
    "- **Ahorro de tiempo:** Los desarrolladores pueden generar funciones, scripts y estructuras b√°sicas en segundos.\n",
    "- **Automatizaci√≥n:** Es posible automatizar tareas repetitivas como generaci√≥n de boilerplate, validaciones o transformaciones de datos.\n",
    "- **Soporte educativo:** Ideal para aprender programaci√≥n, entender errores y practicar con ejemplos guiados.\n",
    "\n",
    "### ‚ö†Ô∏è Desaf√≠os\n",
    "- **Alucinaciones:** El modelo puede inventar funciones o estructuras no v√°lidas, incluso si su sintaxis parece correcta.\n",
    "- **Seguridad:** Puede generar c√≥digo vulnerable si no se aplican filtros o validaciones.\n",
    "- **Dependencia excesiva:** Usar el modelo sin entender el c√≥digo puede llevar a una p√©rdida de criterio t√©cnico o errores en producci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJwgYlgppTpZ"
   },
   "source": [
    "### ü§ó **Hugging Face**\n",
    "\n",
    "**Hugging Face** es una plataforma y comunidad de c√≥digo abierto enfocada en la **inteligencia artificial** y el **procesamiento de lenguaje natural (NLP)**. Es ampliamente reconocida por facilitar el acceso a modelos de lenguaje preentrenados y herramientas para desarrolladores, investigadores y entusiastas de la IA.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß ¬øQu√© ofrece Hugging Face?\n",
    "\n",
    "- **Transformers**: biblioteca en Python para utilizar modelos de lenguaje como BERT, GPT-2, T5, entre otros. Compatible con PyTorch y TensorFlow.\n",
    "- **Model Hub**: repositorio con miles de modelos preentrenados listos para usar.\n",
    "- **Datasets**: colecci√≥n de conjuntos de datos p√∫blicos para entrenamiento y evaluaci√≥n de modelos.\n",
    "- **Spaces**: plataforma para crear y compartir aplicaciones de IA usando Gradio, Streamlit o similares.\n",
    "- **Tokenizers**: herramientas para convertir texto en tokens de forma eficiente, fundamentales en NLP.\n",
    "\n",
    "---\n",
    "\n",
    "#### üí° Aplicaciones comunes\n",
    "\n",
    "- Generaci√≥n de texto\n",
    "- Traducci√≥n autom√°tica\n",
    "- Clasificaci√≥n de sentimientos\n",
    "- Resumen de textos\n",
    "- Chatbots y asistentes virtuales\n",
    "- Pregunta-respuesta (Q&A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "0ccc2ae765614e9aaf34c48f40375421",
      "578ecf7a7d2344d389b152a6d3036905",
      "53e457df3434488db536480814a1b03e",
      "2378ff77415746519068a1de65e02b1a",
      "d3b23cedadc642e5bbbde60b4d2c8581",
      "04c671463f6f4d4cb389bd61ed779e0e",
      "54b8443a901342f8ab8aa020ca846728",
      "99034464723647f48ad2cbc715b49d34",
      "a018ba96a35549b1986c77eabec92b8e",
      "23bb3ea75e914968ba7c114ad18e3713",
      "0d4d00d727694bb490ea0c45b3a3f6f0",
      "a478a29adc08498f85d4d899a6f58407",
      "61f1c124a6ec464cb6680db3b9872535",
      "1afbd95a439746aebd931e1a1a7d8b21",
      "0ab0c0ae041e46d9a84f3134e4e441e0",
      "26605e6d355643499a81ecb7f322b468",
      "8ec30eea604f40d0bcbcec76ad8969e7",
      "16884da87ccf43d68ddec3b5f4bf7afd",
      "454a366b901e404a9ddcb2c4c8f2731d",
      "61ca41bc070744bb8b80109f940a0f0e",
      "e7486777e83843118ab314dbe1a659c8",
      "0997e80f62664c9fbfea036c4c70d05c",
      "8a0f2f135a604f1f99784dc191f5db2d",
      "7f94bde3531146d68800330145085b1e",
      "0864376745694ab8bd4328429790d8de",
      "bc174ce0429d4fe69a4188d406952f59",
      "5af210c91180461b9f204e1aaee7e229",
      "06de7c3c55fa49cc952bc8e8310d6e8d",
      "cc95e12013b54e97adf1fecf1ef77333",
      "6031f0a1b4f84866a81764250f0331b4",
      "93936a6a76434a6b8a3993135d46c073",
      "70b76aa3741244a9bcb40975f74531c2",
      "159d7bfc9df44d51881c6851ec468713",
      "1493afe283d54a2dafd3786aa6765f37",
      "d28df7965af94e51a2e0c494942b5570",
      "2e163197616e4ad98160a31187c46d15",
      "ea18b2fb22534d069726a61644d3f231",
      "bda9071425534eccb13cf43e4d88f0bd",
      "382359fda38f475c9f2020e292dbe584",
      "08e7cc651e0f4d9b90206747b03b9d49",
      "af14cb58a35f4538b057e53bc4b3f576",
      "0f58d3025ecb473b9a516c8c81fdac83",
      "e0fa8a0f692e4ac4b34a3c0438240efc",
      "58201ceb4e1242bab1e74f73d032b0cb",
      "5e21f2b631e8443bb5dae5f96d1cd9d3",
      "109b2bffa35347c68c894dc7e7ea9793",
      "aa76c48d525b451f96eda1d3d875bf41",
      "c06590bc55e749b7bfc95af4d5f296f1",
      "71346fb33abc489cb289bb2cc01ad7a7",
      "c695d2779214421791075d379ff6b19f",
      "13cd635778384ee7ae0c8aa5e31f279b",
      "5e094fd283084f82ae7b802afd563291",
      "9c2d295c1ca1450082de071c71b0c376",
      "5efdda810e4b48eca04b4d1e37b103a8",
      "c0c52d19a5774862a0d9362b5b2e22ff",
      "fb00581ec86646c4acce2f50d80b72fc",
      "19fe410d7b2141e090dde1b5e9cefcb7",
      "ddfb88e242d0437ca9c37fa93dbcb970",
      "a03231b700844cde8e36c530c80623cc",
      "f81fc3565d0b4a15ad53bf44221f0f8b",
      "2e3e4fe21e0847a9b2d4d713c5f62d51",
      "f07b9c01ffd24e01a4dd6307eb6ca4ce",
      "ed3add80891b4c7496d3c4e362f30efb",
      "d19b2fefcab14930869fd9dfd1b7bb5b",
      "bcd132b892d14079bd00baaebc16e002",
      "be07cd73a87a41f78415c13928f6cc9e",
      "7995e59b0d6244b49bb27c63e1427106",
      "b263c32f854a4a0184ac470412d9fb30",
      "f05a23b67c2a456baab0db8a444b8194",
      "9ccaaf2ac10d4966af1e0eb30bbfd235",
      "e7547457df4e458593224e16f81270ef",
      "88beec7f809b4733bd2a586022afc300",
      "bbd068d28f95421e9875518648771d77",
      "fde5a0b5742a455480d0a6037a58ac9f",
      "2da40b0eb9dc48bd95a9b17a612f603f",
      "c8186504cf3b4359800d487b90748a37",
      "e7a5d76d55964cd1abb6adef05fdb56a",
      "aff3387ff0c9470db08d9624870c6a17",
      "3646bb5b202d415aab510b35a045a06b",
      "e93088944d7f449dba67bb784642791f",
      "36add25e16574327a6f4856ffb9782fe",
      "42d2720d91c141bb8b772792f41e1255",
      "19ce3ce8e38746abbaa26615107534d7",
      "81173987b61048c686cbc9ea16749d9a",
      "b68c3b24030f48978b39215cf1c0958a",
      "4129937471bb4c01a9a85d39b048ab46",
      "bc77cde7a2b4442b955960b095317711",
      "b29460c57ff042cda3c7d02a9ef0ff83"
     ]
    },
    "id": "j90SLXQWpcMr",
    "outputId": "1da031b0-933c-4504-fbbc-75a743d32157"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyYbfvx4p_ii"
   },
   "outputs": [],
   "source": [
    "texto_ingles = [\"Hello, how are you?\", \"This is a translation test.\"]\n",
    "tokens = tokenizer(texto_ingles, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WtSq_hXrzeS",
    "outputId": "85a920f5-c12c-4de9-d65e-ff373b1bd780"
   },
   "outputs": [],
   "source": [
    "traduccion_tokens = model.generate(**tokens)\n",
    "traduccion_texto = tokenizer.batch_decode(traduccion_tokens, skip_special_tokens=True)\n",
    "print(traduccion_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8EOs899q7oH",
    "outputId": "a242486e-12cf-4572-e19a-ffca328993f7"
   },
   "outputs": [],
   "source": [
    "for i, t in zip(texto_ingles, traduccion_texto):\n",
    "  print(f\"Ingl√©s: {i} - Espa√±ol: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK_K6kI91Ev9"
   },
   "source": [
    "### üîç **RAG (Retrieval-Augmented Generation)**\n",
    "\n",
    "**RAG (Generaci√≥n Aumentada con Recuperaci√≥n)** es una t√©cnica que combina **modelos generativos** (como los LLMs) con un **sistema de recuperaci√≥n de informaci√≥n**. Su objetivo es mejorar las respuestas del modelo accediendo a fuentes de conocimiento externas en tiempo real.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† ¬øC√≥mo funciona?\n",
    "\n",
    "1. **Consulta**: El usuario env√≠a una pregunta o prompt.\n",
    "2. **Recuperaci√≥n**: El sistema busca informaci√≥n relevante en una base de datos o colecci√≥n de documentos (por ejemplo, usando un motor tipo vectorial como FAISS o Elasticsearch).\n",
    "3. **Generaci√≥n**: El modelo LLM usa la informaci√≥n recuperada para generar una respuesta m√°s precisa y actualizada.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå Caracter√≠sticas clave\n",
    "\n",
    "- No requiere reentrenar el modelo base.\n",
    "- Permite respuestas m√°s **contextualizadas** y **actualizadas**.\n",
    "- Ideal para sistemas que usan datos privados, espec√≠ficos o en constante cambio.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß™ Ejemplos de uso\n",
    "\n",
    "- Asistentes de atenci√≥n al cliente que consultan una base de conocimientos.\n",
    "- Sistemas de recomendaci√≥n basados en documentos.\n",
    "- Aplicaciones empresariales que combinan IA generativa con datos internos.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öñÔ∏è Ventajas vs Fine-Tuning\n",
    "\n",
    "| RAG                                  | Fine-Tuning                          |\n",
    "|--------------------------------------|--------------------------------------|\n",
    "| Usa documentos externos              | Modifica el modelo base              |\n",
    "| No necesita entrenamiento adicional  | Requiere entrenamiento y c√≥mputo     |\n",
    "| Respuestas actualizadas y din√°micas | Respuestas m√°s coherentes y estables |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXKDCr1W1N_w"
   },
   "outputs": [],
   "source": [
    "documentos = {\n",
    "    \"IA\": \"La inteligencia artificial es un campo de la inform√°tica que se centra en la creaci√≥n de sistemas capaces de realizar tareas que normalmente requieren inteligencia humana, como reconocer voz, im√°genes, tomar decisiones o resolver problemas.\",\n",
    "    \"RAG\": \"Retrieval-Augmented Generation (RAG) es una t√©cnica de IA que combina b√∫squeda de informaci√≥n en bases de datos o documentos con modelos generativos, permitiendo respuestas m√°s precisas y basadas en conocimiento actualizado.\",\n",
    "    \"Machine Learning\": \"El aprendizaje autom√°tico es una rama de la inteligencia artificial que permite a las computadoras aprender de datos y mejorar su rendimiento sin ser programadas expl√≠citamente para cada tarea.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCjfwGN61_tb"
   },
   "outputs": [],
   "source": [
    "def recuperar_contexto(pregunta):\n",
    "  for tema, contenido in documentos.items():\n",
    "    if tema.lower() in pregunta.lower():\n",
    "      return contenido\n",
    "    return \"No se encontro informaci√≥n relevante en la base de conocimientos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIQK1JfH2RcI"
   },
   "outputs": [],
   "source": [
    "def generar_respuesta(pregunta):\n",
    "  contexto = recuperar_contexto(pregunta)\n",
    "  prompt = f\"\"\"Usa el siguiente contexto para responder la pregunta de manera clara y concisa \\n\\n Contexto: {contexto} \\n\\n Pregunta: {pregunta}\"\"\"\n",
    "  genai.configure(api_key=GEMINI_API_KEY)\n",
    "  model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "  response = model.generate_content(prompt)\n",
    "  return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "fPpoyoG2257l",
    "outputId": "bf467fb0-68ce-4e72-f704-e0f04cd3aa0f"
   },
   "outputs": [],
   "source": [
    "pregunta_usuario = \"Que es RAG\"\n",
    "respuesta = generar_respuesta(pregunta_usuario)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "g4T_wd7M4A8B",
    "outputId": "019a581a-baa7-40c0-9826-90282ee4c145"
   },
   "outputs": [],
   "source": [
    "pregunta_usuario = \"Que es Cartagena\"\n",
    "respuesta = generar_respuesta(pregunta_usuario)\n",
    "print(respuesta)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
